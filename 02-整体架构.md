# 02. 整体架构

---

## 2.1 系统架构图

```
┌─────────────────────────────────────────────────────────────────┐
│                       Kubernetes Cluster                         │
│                                                                  │
│  ┌────────────────────────────────────────────────────────┐    │
│  │  Ingress / LoadBalancer                                │    │
│  │  - HTTPS Termination                                   │    │
│  │  - gRPC/HTTP2 Support                                  │    │
│  └────────────┬───────────────────────────────────────────┘    │
│               │                                                  │
│  ┌────────────▼────────────────────────────────────────────┐   │
│  │  SyncTV Service (Headless)                             │   │
│  │  - 所有副本完全平等                                    │   │
│  │  - 无全局角色分配                                      │   │
│  └────────────┬────────────────────────────────────────────┘   │
│               │                                                  │
│  ┌────────────┴─────────┬──────────┬──────────┬─────────┐     │
│  │                      │          │          │         │     │
│  ▼                      ▼          ▼          ▼         ▼     │
│  synctv-0              synctv-1  synctv-2  synctv-3  synctv-4 │
│  (副本)                (副本)    (副本)    (副本)    (副本)   │
│  ├─ gRPC Server        ├─ ...    ├─ ...    ├─ ...    ├─ ...   │
│  │  ├─ Streaming (实时)└─ ...    └─ ...    └─ ...    └─ ...   │
│  │  └─ HTTP/JSON (管理)                                        │
│  ├─ RTMP Server                                                │
│  ├─ HLS/FLV Server                                             │
│  └─ gRPC Client (节点间)                                       │
│                                                                  │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  共享数据层                                              │  │
│  │  ┌─────────────────┐  ┌──────────────────┐             │  │
│  │  │  PostgreSQL     │  │  Redis Cluster   │             │  │
│  │  │  - 持久化数据   │  │  - 流注册信息    │             │  │
│  │  │  - 用户/房间    │  │  - 事件广播      │             │  │
│  │  │  - 聊天记录     │  │  - 临时缓存      │             │  │
│  │  └─────────────────┘  └──────────────────┘             │  │
│  └──────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────┘
```

---

## 2.2 多副本架构原理

### 2.2.1 平等副本设计

**核心理念**：所有副本完全平等，无全局角色

```rust
// ✅ 所有副本完全平等
// ✅ 没有 Master/Slave 角色
// ✅ 没有一致性哈希选举
// ✅ 没有全局角色分配
// ✅ 只通过 Redis 原子操作 (HSETNX) 注册 Publisher

pub struct Replica {
    pub node_id: String,          // 节点ID (Pod名称)
    pub address: String,          // 节点地址
    pub capabilities: Capabilities,
}

// 副本能力（所有副本能力相同）
pub struct Capabilities {
    pub can_publish: bool,        // 可接受推流
    pub can_subscribe: bool,      // 可提供拉流
    pub max_streams: usize,       // 最大流数量
}
```

**关键特点**：

- ✅ 所有副本功能完全一致
- ✅ 任何副本都可以接受推流
- ✅ 任何副本都可以提供拉流
- ✅ 副本间通过 gRPC 互相通信

### 2.2.2 直播流处理模型

#### 推流处理

```
用户推流请求
    │
    │ RTMP Publish: rtmp://synctv.io/live/room_123_key_abc
    ▼
┌─────────────────────────────────┐
│  任意副本 (例如 synctv-2)       │
│                                 │
│  1. 验证 stream_key             │
│  2. 原子注册 Publisher          │
│     (Redis HSETNX - 保证唯一)   │
│  3. 启动 RTMP 接收              │
│  4. 生成 HLS 切片               │
│  5. 维护心跳到 Redis            │
│     stream:room_123 → {        │
│       "publisher_node": "synctv-2",
│       "stream_key": "room_123_key_abc",
│       "started_at": 1706000000,
│       "status": "live",        │
│       "last_heartbeat": 1706000150
│     }                           │
│                                 │
│  注意: 不广播事件用于协调       │
│        其他副本通过 Redis 查询   │
│        在收到FLV请求时懒加载拉流 │
│                                 │
└─────────────────────────────────┘
```

**Redis 流注册结构**：

```redis
# 流信息注册 (Hash)
HSET stream:room_123
  publisher_node "synctv-2"
  stream_key "room_123_key_abc"
  started_at "1706000000"
  status "live"
  viewer_count "0"

# TTL 自动过期 (5分钟无心跳则删除)
EXPIRE stream:room_123 300

# 流列表 (Set，方便查询所有活跃流)
SADD active_streams "room_123"
```

#### 拉流处理

**设计原则**：

- **HLS**: 全部从 Publisher 节点获取,确保切片一致性
- **FLV**: 拉流节点主动从 Publisher 拉取 RTMP 流,在本地生成 FLV 分发
- **TS 存储**: HLS 切片可选存储到 OSS,支持 CDN 分发

##### HLS 拉流 (从 Publisher 获取)

```
客户端请求 HLS
    │
    │ GET https://synctv.io/hls/room_123/index.m3u8
    ▼
┌─────────────────────────────────┐
│  任意副本 (例如 synctv-4)       │
│                                 │
│  1. 查询 Redis: stream:room_123 │
│     → publisher_node = "synctv-2"
│                                 │
│  2. 检查 OSS 存储 (如果启用)    │
│     └─ 返回 OSS URL             │
│                                 │
│  3. 代理到 Publisher 节点       │
│     透传请求到 synctv-2         │
│     → synctv-2 返回切片         │
│                                 │
│  4. 返回给客户端                │
│     (可选: 本地缓存 1 分钟)     │
│                                 │
└─────────────────────────────────┘
```

**优势**：

- ✅ 确保所有客户端获取的 HLS 切片一致
- ✅ 避免多节点各自生成 HLS 导致的不一致
- ✅ 支持 OSS 存储,降低 Publisher 压力

##### FLV 拉流 (主动拉取模式)

```
客户端请求 FLV
    │
    │ GET https://synctv.io/flv/room_123.flv
    ▼
┌─────────────────────────────────┐
│  任意副本 (例如 synctv-4)       │
│                                 │
│  1. 查询 Redis: stream:room_123 │
│     → publisher_node = "synctv-2"
│                                 │
│  2. 检查本地是否已建立拉流      │
│     └─ 已建立 → 订阅本地流      │
│                                 │
│  3. 未建立 → 主动拉流           │
│     ┌──────────────────────┐   │
│     │ 启动 RTMP Client     │   │
│     │ 连接到 synctv-2      │   │
│     │ 订阅 room_123 流     │   │
│     └──────────────────────┘   │
│                                 │
│  4. 接收 RTMP 流数据            │
│     └─ 转换为 FLV 格式          │
│                                 │
│  5. 分发给客户端                │
│     (多个客户端共享同一拉流)    │
│                                 │
└─────────────────────────────────┘
```

**优势**：

- ✅ 拉流节点主动拉取,类似推流机制
- ✅ 在本地生成 FLV,降低网络传输
- ✅ 多个客户端共享同一拉流连接
- ✅ 拉流节点可以做本地缓存(GOP 缓存)

**代码实现**：

##### HLS 请求处理

```rust
pub async fn handle_hls_request(
    Path(room_id): Path<String>,  // nanoid(12)
    Path(segment): Path<String>,
    State(state): State<AppState>,
) -> Result<Response> {
    // 1. 查询 Publisher 节点
    let stream_key = format!("room_{}", room_id);
    let publisher_info = state.redis
        .hgetall(format!("stream:{}", stream_key))
        .await?;

    let publisher_node = publisher_info
        .get("publisher_node")
        .ok_or(Error::StreamNotFound)?;

    // 2. 检查 OSS 存储 (如果启用)
    if let Some(oss_url) = state.oss_storage.get_segment_url(&stream_key, &segment).await? {
        return Ok(Response::builder()
            .status(StatusCode::FOUND)
            .header("Location", oss_url)
            .header("X-Storage", "OSS")
            .body(Body::empty())?);
    }

    // 3. 本地是 Publisher
    if publisher_node == &state.local_node_id {
        let content = read_local_hls_segment(&stream_key, &segment).await?;
        return Ok(Response::builder()
            .header("X-Cache", "LOCAL")
            .header("X-Publisher-Node", publisher_node)
            .body(content.into())?);
    }

    // 4. 透传到 Publisher 节点
    let publisher_addr = state.get_node_address(publisher_node).await?;
    let proxy_url = format!("http://{}/hls/{}/{}", publisher_addr, room_id, segment);

    let content = state.http_client
        .get(&proxy_url)
        .timeout(Duration::from_secs(5))
        .send()
        .await?
        .bytes()
        .await?;

    // 5. 可选: 短时间本地缓存 (1分钟)
    let cache_key = format!("hls:{}:{}", stream_key, segment);
    state.local_cache.insert(cache_key, Arc::new(content.clone()), 60).await;

    Ok(Response::builder()
        .header("X-Cache", "PROXY")
        .header("X-Publisher-Node", publisher_node)
        .body(content.into())?)
}
```

##### FLV 请求处理 (懒加载)

**关键**: 收到客户端请求时才创建拉流,而非 Publisher 主动通知

```rust
pub async fn handle_flv_request(
    Path(room_id): Path<String>,  // nanoid(12)
    State(state): State<AppState>,
) -> Result<Response> {
    // 1. 查询 Publisher 节点
    let stream_key = format!("room_{}", room_id);
    let publisher_info = state.redis
        .hgetall(format!("stream:{}", stream_key))
        .await?;

    let publisher_node = publisher_info
        .get("publisher_node")
        .ok_or(Error::StreamNotFound)?;

    // 2. 懒加载: 收到请求时才创建拉流
    //    - 如果本地已存在拉流 → 直接订阅
    //    - 如果本地不存在 → 懒加载创建 RTMP Client,连接 Publisher
    let pull_stream_manager = state.pull_stream_manager.clone();
    let local_stream = pull_stream_manager
        .get_or_create_pull_stream(
            &stream_key,
            publisher_node,
            state.local_node_id.clone(),
        )
        .await?;

    tracing::info!(
        stream_key = %stream_key,
        publisher_node = %publisher_node,
        subscribers = local_stream.subscriber_count(),
        "Client subscribed to FLV stream"
    );

    // 3. 订阅本地流,生成 FLV
    let flv_stream = local_stream.subscribe_flv().await?;

    // 4. 返回 FLV 流
    let body = Body::from_stream(flv_stream);

    Ok(Response::builder()
        .header("Content-Type", "video/x-flv")
        .header("X-Publisher-Node", publisher_node)
        .header("X-Pull-Mode", "lazy-load")
        .body(body)?)
}

/// 拉流管理器 (懒加载模式)
pub struct PullStreamManager {
    // stream_key -> PullStream
    streams: Arc<DashMap<String, Arc<PullStream>>>,
}

impl PullStreamManager {
    /// 懒加载: 获取或创建拉流 (仅在客户端请求 FLV 时触发)
    pub async fn get_or_create_pull_stream(
        &self,
        stream_key: &str,
        publisher_node: &str,
        local_node_id: String,
    ) -> Result<Arc<PullStream>> {
        // 1. 检查是否已存在健康的拉流
        if let Some(stream) = self.streams.get(stream_key) {
            if stream.is_healthy().await {
                tracing::debug!(
                    stream_key = %stream_key,
                    subscribers = stream.subscriber_count(),
                    "Reusing existing pull stream"
                );
                return Ok(stream.clone());
            } else {
                // 移除不健康的拉流
                self.streams.remove(stream_key);
            }
        }

        // 2. 懒加载: 创建新的拉流连接
        tracing::info!(
            stream_key = %stream_key,
            publisher_node = %publisher_node,
            "Lazy loading: Creating pull stream on first FLV request"
        );

        let publisher_addr = format!("{}:1935", publisher_node);
        let rtmp_url = format!("rtmp://{}/live/{}", publisher_addr, stream_key);

        // 创建 RTMP Client
        let rtmp_client = RtmpClient::connect(&rtmp_url).await?;

        let pull_stream = Arc::new(PullStream::new(
            stream_key.to_string(),
            rtmp_client,
            local_node_id,
        ));

        // 启动拉流任务
        pull_stream.start().await?;

        // 注册自动清理任务
        Self::register_cleanup_task(
            stream_key.to_string(),
            pull_stream.clone(),
            self.streams.clone(),
        );

        // 存储到管理器
        self.streams.insert(stream_key.to_string(), pull_stream.clone());

        Ok(pull_stream)
    }

    /// 注册自动清理任务 (无订阅者时自动停止拉流)
    fn register_cleanup_task(
        stream_key: String,
        pull_stream: Arc<PullStream>,
        streams: Arc<DashMap<String, Arc<PullStream>>>,
    ) {
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(60));

            loop {
                interval.tick().await;

                // 检查订阅者数量
                if pull_stream.subscriber_count() == 0 {
                    let idle_time = pull_stream.last_active_time().elapsed();

                    // 无订阅者超过 5 分钟,停止拉流
                    if idle_time > Duration::from_secs(300) {
                        tracing::info!(
                            stream_key = %stream_key,
                            idle_time = ?idle_time,
                            "Auto cleanup: Stopping pull stream (no subscribers for 5 min)"
                        );

                        pull_stream.stop().await.ok();
                        streams.remove(&stream_key);
                        break;
                    }
                } else {
                    // 有订阅者,更新最后活跃时间
                    pull_stream.update_last_active_time();
                }
            }
        });
    }
}

/// 拉流实例
pub struct PullStream {
    stream_key: String,
    rtmp_client: Arc<RtmpClient>,
    local_node_id: String,

    // GOP 缓存
    gop_cache: Arc<RwLock<Vec<Arc<Bytes>>>>,

    // 订阅者列表
    subscribers: Arc<RwLock<Vec<mpsc::Sender<Arc<Bytes>>>>>,
}

impl PullStream {
    /// 启动拉流
    pub async fn start(&self) -> Result<()> {
        let rtmp_client = self.rtmp_client.clone();
        let gop_cache = self.gop_cache.clone();
        let subscribers = self.subscribers.clone();

        tokio::spawn(async move {
            loop {
                // 从 RTMP 接收数据包
                match rtmp_client.read_packet().await {
                    Ok(packet) => {
                        let data = Arc::new(packet.data);

                        // 更新 GOP 缓存
                        if packet.is_key_frame {
                            let mut cache = gop_cache.write().await;
                            cache.clear();
                            cache.push(data.clone());
                        } else {
                            gop_cache.write().await.push(data.clone());
                        }

                        // 分发给订阅者
                        let subs = subscribers.read().await;
                        for sub in subs.iter() {
                            let _ = sub.send(data.clone()).await;
                        }
                    }
                    Err(e) => {
                        tracing::error!(error = ?e, "Pull stream error");
                        break;
                    }
                }
            }
        });

        Ok(())
    }

    /// 订阅 FLV 流
    pub async fn subscribe_flv(&self) -> Result<(impl Stream<Item = Result<Bytes>>, CancellationToken)> {
        let (tx, rx) = mpsc::channel(100);

        // 发送 GOP 缓存
        let gop_cache = self.gop_cache.read().await;
        for packet in gop_cache.iter() {
            tx.send(packet.clone()).await.ok();
        }

        // 添加到订阅者列表
        self.subscribers.write().await.push(tx);

        let cancel_token = CancellationToken::new();

        let stream = ReceiverStream::new(rx)
            .map(|data| Ok((*data).clone()));

        Ok((stream, cancel_token))
    }
}
```

### 2.2.3 OSS 存储集成 (可选)

为了降低 Publisher 节点的带宽压力,支持将 HLS 切片上传到 OSS 等对象存储。

#### 存储接口设计

```rust
/// 远程存储接口
#[async_trait]
pub trait RemoteStorage: Send + Sync {
    /// 上传文件
    async fn upload(
        &self,
        key: &str,
        data: Bytes,
        content_type: &str,
    ) -> Result<String>;  // 返回访问URL

    /// 获取文件URL
    async fn get_url(&self, key: &str) -> Result<Option<String>>;

    /// 删除文件
    async fn delete(&self, key: &str) -> Result<()>;

    /// 批量删除
    async fn delete_batch(&self, keys: Vec<String>) -> Result<()>;
}

/// AWS S3 实现
pub struct S3Storage {
    client: aws_sdk_s3::Client,
    bucket: String,
    region: String,
    cdn_domain: Option<String>,
}

#[async_trait]
impl RemoteStorage for S3Storage {
    async fn upload(
        &self,
        key: &str,
        data: Bytes,
        content_type: &str,
    ) -> Result<String> {
        self.client
            .put_object()
            .bucket(&self.bucket)
            .key(key)
            .body(data.into())
            .content_type(content_type)
            .send()
            .await?;

        // 返回 CDN URL (如果配置了 CDN)
        if let Some(cdn_domain) = &self.cdn_domain {
            Ok(format!("https://{}/{}", cdn_domain, key))
        } else {
            Ok(format!(
                "https://{}.s3.{}.amazonaws.com/{}",
                self.bucket, self.region, key
            ))
        }
    }

    async fn get_url(&self, key: &str) -> Result<Option<String>> {
        // 检查文件是否存在
        match self.client
            .head_object()
            .bucket(&self.bucket)
            .key(key)
            .send()
            .await
        {
            Ok(_) => {
                if let Some(cdn_domain) = &self.cdn_domain {
                    Ok(Some(format!("https://{}/{}", cdn_domain, key)))
                } else {
                    Ok(Some(format!(
                        "https://{}.s3.{}.amazonaws.com/{}",
                        self.bucket, self.region, key
                    )))
                }
            }
            Err(_) => Ok(None),
        }
    }

    async fn delete(&self, key: &str) -> Result<()> {
        self.client
            .delete_object()
            .bucket(&self.bucket)
            .key(key)
            .send()
            .await?;
        Ok(())
    }

    async fn delete_batch(&self, keys: Vec<String>) -> Result<()> {
        if keys.is_empty() {
            return Ok(());
        }

        let objects = keys
            .into_iter()
            .map(|key| {
                aws_sdk_s3::types::ObjectIdentifier::builder()
                    .key(key)
                    .build()
            })
            .collect::<Result<Vec<_>, _>>()?;

        self.client
            .delete_objects()
            .bucket(&self.bucket)
            .delete(
                aws_sdk_s3::types::Delete::builder()
                    .set_objects(Some(objects))
                    .build()?,
            )
            .send()
            .await?;

        Ok(())
    }
}

/// 阿里云 OSS 实现
pub struct AliyunOssStorage {
    client: aliyun_oss_client::Client,
    bucket: String,
    endpoint: String,
    cdn_domain: Option<String>,
}

#[async_trait]
impl RemoteStorage for AliyunOssStorage {
    async fn upload(
        &self,
        key: &str,
        data: Bytes,
        content_type: &str,
    ) -> Result<String> {
        self.client
            .put_object()
            .bucket(&self.bucket)
            .object(key)
            .body(data)
            .content_type(content_type)
            .send()
            .await?;

        if let Some(cdn_domain) = &self.cdn_domain {
            Ok(format!("https://{}/{}", cdn_domain, key))
        } else {
            Ok(format!("https://{}.{}/{}", self.bucket, self.endpoint, key))
        }
    }

    async fn get_url(&self, key: &str) -> Result<Option<String>> {
        match self.client
            .head_object()
            .bucket(&self.bucket)
            .object(key)
            .send()
            .await
        {
            Ok(_) => {
                if let Some(cdn_domain) = &self.cdn_domain {
                    Ok(Some(format!("https://{}/{}", cdn_domain, key)))
                } else {
                    Ok(Some(format!("https://{}.{}/{}", self.bucket, self.endpoint, key)))
                }
            }
            Err(_) => Ok(None),
        }
    }

    async fn delete(&self, key: &str) -> Result<()> {
        self.client
            .delete_object()
            .bucket(&self.bucket)
            .object(key)
            .send()
            .await?;
        Ok(())
    }

    async fn delete_batch(&self, keys: Vec<String>) -> Result<()> {
        // 阿里云 OSS 批量删除
        for key in keys {
            self.delete(&key).await?;
        }
        Ok(())
    }
}
```

#### Publisher 上传 HLS 切片

```rust
/// HLS 切片生成时上传到 OSS
pub async fn on_hls_segment_created(
    stream_key: &str,
    segment_name: &str,
    segment_data: Bytes,
    storage: &dyn RemoteStorage,
) -> Result<String> {
    let storage_key = format!("hls/{}/{}", stream_key, segment_name);

    // 上传到 OSS
    let url = storage
        .upload(&storage_key, segment_data, "video/mp2t")
        .await?;

    tracing::info!(
        stream_key = %stream_key,
        segment_name = %segment_name,
        url = %url,
        "Uploaded HLS segment to OSS"
    );

    Ok(url)
}

/// 流结束时清理 OSS 文件
pub async fn on_stream_ended(
    stream_key: &str,
    storage: &dyn RemoteStorage,
) -> Result<()> {
    // 列出该流的所有切片
    let prefix = format!("hls/{}/", stream_key);

    // 删除所有切片 (延迟删除,保留1小时)
    tokio::spawn(async move {
        tokio::time::sleep(Duration::from_secs(3600)).await;

        // 删除逻辑 (需要列出对象后批量删除)
        // storage.delete_batch(keys).await
    });

    Ok(())
}
```

**配置示例**：

```toml
[live.storage]
enabled = true
provider = "s3"  # "s3" | "aliyun_oss" | "local"

[live.storage.s3]
bucket = "synctv-hls"
region = "us-west-2"
access_key = "AKIAIOSFODNN7EXAMPLE"
secret_key = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
cdn_domain = "cdn.synctv.io"  # 可选: 使用 CDN 加速

[live.storage.aliyun_oss]
bucket = "synctv-hls"
endpoint = "oss-cn-hangzhou.aliyuncs.com"
access_key_id = "LTAI4FnKxXXXXXXXX"
access_key_secret = "xxxxxxxxxxxxxx"
cdn_domain = "cdn.synctv.cn"
```

### 2.2.4 副本故障处理

#### Publisher 故障检测

```rust
pub async fn detect_publisher_failure(
    stream_key: &str,
    redis: &RedisClient,
) -> Result<bool> {
    let stream_info = redis
        .hgetall(format!("stream:{}", stream_key))
        .await?;
    
    if stream_info.is_empty() {
        // Redis 中无记录，认为流已结束或超时
        return Ok(true);
    }
    
    // 检查最后心跳时间
    if let Some(last_heartbeat) = stream_info.get("last_heartbeat") {
        let last_time: i64 = last_heartbeat.parse()?;
        let now = Utc::now().timestamp();
        
        // 超过 30 秒无心跳认为故障
        if now - last_time > 30 {
            return Ok(true);
        }
    }
    
    Ok(false)
}
```

#### Publisher 心跳维护

```rust
pub async fn maintain_publisher_heartbeat(
    stream_key: String,
    local_node_id: String,
    redis: Arc<RedisClient>,
) {
    tokio::spawn(async move {
        let mut interval = tokio::time::interval(Duration::from_secs(10));
        
        loop {
            interval.tick().await;
            
            // 更新心跳时间
            let result = redis.hset(
                format!("stream:{}", stream_key),
                "last_heartbeat",
                Utc::now().timestamp().to_string(),
            ).await;
            
            if result.is_err() {
                warn!("Failed to update heartbeat for stream: {}", stream_key);
                break;
            }
            
            // 刷新 TTL
            redis.expire(format!("stream:{}", stream_key), 300).await.ok();
        }
    });
}
```

#### 故障转移策略

**简化方案**：Publisher 故障时，流自动中断

```
Publisher 故障
    │
    ▼
Redis TTL 过期 (5分钟)
    │
    ▼
stream:room_123 自动删除
    │
    ▼
客户端拉流失败 (404)
    │
    ▼
客户端提示 "直播已结束"
    │
    ▼
推流用户重新推流到其他副本
```

**优点**：

- ✅ 实现简单，无复杂的故障转移逻辑
- ✅ 无需分布式锁
- ✅ 无需状态迁移

**缺点**：

- ❌ Publisher 故障时流会中断

**改进方案（可选）**：自动重推

```rust
// 其他副本可以尝试从原 Publisher 拉取并重新推流
// 但实现复杂度较高，建议后续版本实现
```

---

## 2.3 服务发现机制

### 2.3.1 Kubernetes 服务发现

**Headless Service**：

```yaml
apiVersion: v1
kind: Service
metadata:
  name: synctv
  namespace: synctv
spec:
  clusterIP: None  # Headless
  selector:
    app: synctv
  ports:
  - name: http
    port: 8080
  - name: rtmp
    port: 1935
```

**DNS 查询**：

```rust
use trust_dns_resolver::TokioAsyncResolver;

pub async fn discover_replicas() -> Result<Vec<String>> {
    let resolver = TokioAsyncResolver::tokio_from_system_conf()?;
    
    // 查询 SRV 记录
    let response = resolver.srv_lookup("_http._tcp.synctv.synctv.svc.cluster.local").await?;
    
    let mut replicas = Vec::new();
    for srv in response.iter() {
        let target = srv.target().to_string().trim_end_matches('.').to_string();
        replicas.push(target);
    }
    
    Ok(replicas)
}
```

**环境变量方式（更简单）**：

```rust
pub async fn discover_replicas_simple() -> Result<Vec<String>> {
    let service_name = env::var("SERVICE_NAME")?;  // "synctv"
    let namespace = env::var("NAMESPACE")?;        // "synctv"
    
    // Kubernetes 会注入所有 Pod 的环境变量
    // SYNCTV_SERVICE_HOST_0, SYNCTV_SERVICE_HOST_1, ...
    
    let mut replicas = Vec::new();
    for i in 0..10 {  // 假设最多 10 个副本
        if let Ok(host) = env::var(format!("{}_SERVICE_HOST_{}", service_name.to_uppercase(), i)) {
            replicas.push(host);
        }
    }
    
    Ok(replicas)
}
```

### 2.3.2 节点注册与心跳

**节点注册到 Redis**：

```rust
pub async fn register_node(
    node_id: String,
    address: String,
    redis: &RedisClient,
) -> Result<()> {
    let node_info = serde_json::json!({
        "node_id": node_id,
        "address": address,
        "started_at": Utc::now().timestamp(),
        "status": "online",
    });
    
    redis.hset(
        "cluster:nodes",
        &node_id,
        node_info.to_string(),
    ).await?;
    
    redis.expire("cluster:nodes", 300).await?;
    
    Ok(())
}

pub async fn maintain_node_heartbeat(
    node_id: String,
    redis: Arc<RedisClient>,
) {
    tokio::spawn(async move {
        let mut interval = tokio::time::interval(Duration::from_secs(10));
        
        loop {
            interval.tick().await;
            
            redis.hset(
                "cluster:nodes",
                &node_id,
                format!(r#"{{"last_heartbeat": {}}}"#, Utc::now().timestamp()),
            ).await.ok();
        }
    });
}
```

---

## 2.4 负载均衡策略

### 2.4.1 推流负载均衡

**策略**：客户端层面轮询

```
客户端获取推流地址
    │
    │ GET /api/v1/rooms/{room_id}/live/publish-url
    ▼
服务端返回：
  {
    "rtmp_url": "rtmp://synctv-2.synctv.svc.cluster.local/live",
    "stream_key": "room_123_key_abc"
  }
    │
    ▼
客户端推流到指定副本
```

**后端实现**：

```rust
pub async fn get_publish_url(
    room_id: &str,  // nanoid(12)
    state: &AppState,
) -> Result<PublishUrlResponse> {
    // 简单轮询选择副本（使用room_id的哈希值）
    let replicas = state.get_all_replicas().await?;
    let hash = calculate_hash(room_id);
    let selected = &replicas[hash % replicas.len()];

    let stream_key = generate_stream_key(room_id);
    
    Ok(PublishUrlResponse {
        rtmp_url: format!("rtmp://{}/live", selected),
        stream_key,
    })
}
```

### 2.4.2 拉流负载均衡

**策略**：Kubernetes Service 自动负载均衡

```
客户端请求播放
    │
    │ GET https://synctv.io/hls/room_123/index.m3u8
    ▼
Ingress/Service 自动分配到任意副本
    │
    ▼
副本查询 Publisher 并拉取数据
```

**HLS 播放地址不包含副本信息**：

```json
{
  "hls_url": "https://synctv.io/hls/room_123/index.m3u8",
  "flv_url": "https://synctv.io/flv/room_123.flv"
}
```

### 2.4.3 gRPC Streaming 负载均衡 (Redis 消息队列)

**设计原则**：使用 gRPC bidirectional streaming 替代 WebSocket，配合 Redis 实现跨节点消息分发

客户端通过 gRPC streaming 连接到任意副本，消息通过 Redis Pub/Sub 分发到所有相关副本：

```
客户端 gRPC Streaming 连接
    │
    ├─ Client A → Node-1 (MessageStream RPC)
    ├─ Client B → Node-2 (MessageStream RPC)
    ├─ Client C → Node-1 (MessageStream RPC)
    └─ Client D → Node-3 (MessageStream RPC)

房间消息分发:
    │
    │ Client A 发送 ClientMessage (Protobuf)
    ▼
Node-1 接收消息
    │
    │ 1. 验证 Protobuf 格式
    │ 2. 处理业务逻辑
    │ 3. 存储到数据库 (如需要)
    ▼
Redis Pub/Sub
    │
    │ PUBLISH room:123:messages (JSON序列化)
    ▼
所有订阅该房间的节点
    │
    ├─ Node-1 → 反序列化 → 推送 ServerMessage (Protobuf) → Client A, C
    ├─ Node-2 → 反序列化 → 推送 ServerMessage (Protobuf) → Client B
    └─ Node-3 → 反序列化 → 推送 ServerMessage (Protobuf) → Client D
```

**实现代码**：

```rust
// gRPC MessageStream 实现
#[tonic::async_trait]
impl ClientService for MyClientService {
    type MessageStreamStream = ReceiverStream<Result<ServerMessage, Status>>;

    async fn message_stream(
        &self,
        request: Request<tonic::Streaming<ClientMessage>>,
    ) -> Result<Response<Self::MessageStreamStream>, Status> {
        let mut client_stream = request.into_inner();
        let (tx, rx) = mpsc::channel(100);

        let room_hub = self.room_hub.clone();
        let redis_client = self.redis_client.clone();

        tokio::spawn(async move {
            let mut current_room: Option<String> = None;

            while let Some(msg) = client_stream.message().await? {
                match msg.payload {
                    Some(client_message::Payload::JoinRoom(join)) => {
                        current_room = Some(join.room_id.clone());
                        // 注册到房间，订阅 Redis 频道
                        room_hub.join_room(join.room_id, tx.clone()).await?;
                    }
                    Some(client_message::Payload::Chat(chat)) => {
                        // 发布消息到 Redis
                        let channel = format!("room:{}:messages", chat.room_id);
                        let payload = serde_json::to_string(&chat)?;
                        publish_to_redis(&redis_client, &channel, payload).await?;
                    }
                    _ => {}
                }
            }

            // 清理：离开房间
            if let Some(room_id) = current_room {
                room_hub.leave_room(&room_id).await?;
            }

            Ok::<_, anyhow::Error>(())
        });

        Ok(Response::new(ReceiverStream::new(rx)))
    }
}
```

**优势**：

- ✅ **类型安全**: Protobuf 编译期检查，减少运行时错误
- ✅ **性能更优**: 二进制编码，比 JSON 更高效
- ✅ **自动流控**: HTTP/2 内置背压机制
- ✅ **自动重连**: tonic 客户端库自动处理断线重连
- ✅ **无亲和性**: 客户端可连接任意节点
- ✅ **水平扩展**: 简化负载均衡配置

---

## 2.5 数据一致性

### 2.5.1 强一致性数据（PostgreSQL）

- 用户信息
- 房间配置
- 权限设置
- 聊天历史（最近500条）

**特点**：

- 所有副本读写同一个 PostgreSQL
- 数据库层面保证一致性
- 使用事务保证原子性

### 2.5.2 最终一致性数据（Redis Pub/Sub）

- 播放状态
- 成员在线状态
- 实时消息

**特点**：

- 通过 Redis Pub/Sub 广播
- 副本间可能有短暂延迟（< 100ms）
- 客户端通过 gRPC streaming 接收更新

### 2.5.3 临时数据（内存）

- 弹幕（最近10秒）
- HLS 缓存（最近1分钟）
- 连接状态

**特点**：

- 仅存在于内存
- 副本间不同步
- 无持久化

---

## 2.6 架构优势

### ✅ 简单性

- 无复杂的 Master 选举逻辑
- 无一致性哈希
- 无状态迁移

### ✅ 可扩展性

- 副本数量可随意增减
- 水平扩展简单（HPA）
- 无单点瓶颈

### ✅ 可靠性

- 副本故障不影响其他副本
- 数据库和 Redis 是唯一的共享状态
- 故障恢复简单（重启即可）

### ✅ 性能

- 本地缓存减少跨节点通信
- gRPC 高效的流传输
- 零拷贝设计

### ✅ 直播流分发优化

- **HLS 一致性**: 全部从 Publisher 获取,确保切片一致性
- **FLV 主动拉流**: 拉流节点主动拉取 RTMP,降低 Publisher 压力
- **OSS 存储**: 支持将 HLS 切片上传到对象存储,CDN 加速
- **GOP 缓存**: 拉流节点本地 GOP 缓存,快速首帧
- **多客户端共享**: 同一节点多个客户端共享一个拉流连接

---

**下一章**: [03-核心模块](./03-核心模块.md)
