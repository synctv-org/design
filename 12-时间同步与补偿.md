# 21. 时间同步与补偿

---

## 目录

- [1. 问题背景](#1-问题背景)
- [2. HTTP时间同步](#2-http时间同步)
- [3. 时间补偿原理](#3-时间补偿原理)
- [4. 补偿算法](#4-补偿算法)
- [5. 实现方案](#5-实现方案)
- [6. 性能优化](#6-性能优化)

---

## 1. 问题背景

### 1.1 为什么需要时间同步

多人实时观看场景中,播放状态同步的挑战:

- **网络延迟**: 不同用户网络延迟不同 (10ms-1000ms+)
- **时钟偏移**: 客户端与服务端时钟可能不同步
- **操作延迟**: 用户操作 (播放/暂停/seek) 到其他用户看到存在延迟

**目标**: 保证所有用户视频进度误差 < 300ms (人眼察觉阈值)

### 1.2 同步场景

| 场景 | 要求 | 挑战 |
|------|------|------|
| 播放/暂停 | 所有用户几乎同时播放/暂停 | 网络延迟差异 |
| 进度同步 (seek) | 所有用户跳转到相同位置 | 缓冲时间不同 |
| 断线重连 | 重连后追赶当前进度 | 计算正确的追赶时间 |
| 新用户加入 | 加入时同步到当前进度 | 初始化时间计算 |

---

## 2. HTTP时间同步

### 2.1 为什么需要时间同步

**问题**: 当前设计假设客户端和服务端时钟同步,但实际情况:

- 客户端设备时间可能不准确 (用户手动修改、时区错误、系统时间漂移)
- 如果客户端时钟慢5秒,计算的网络延迟会被高估5秒
- 如果客户端时钟快5秒,计算的延迟会是负数

**解决方案**: 客户端启动时通过HTTP协议与服务端同步时间,计算时钟偏移量

### 2.2 HTTP时间同步协议

#### 2.2.1 API设计

```http
GET /api/v1/time/sync
Authorization: Bearer {token}  // 可选,未登录也可调用

Response: 200 OK
{
  "code": 0,
  "message": "success",
  "data": {
    "server_time": 1706174400000,      // 服务器当前时间 (毫秒)
    "server_recv_time": 1706174400000, // 服务器接收请求时间
    "server_send_time": 1706174400001, // 服务器发送响应时间
    "request_id": "uuid"               // 用于多次测量去重
  }
}
```

#### 2.2.2 时间同步算法 (类似NTP)

时间同步算法基于NTP协议原理：

- 多次测量取中位数以去除异常值
- 计算时钟偏移: `offset = ((t2 - t1) + (t3 - t4)) / 2`
- 计算往返延迟: `delay = (t4 - t1) - (t3 - t2)`
- 定期重新同步（每5分钟）以应对时间漂移

#### 2.2.3 服务端实现

```rust
use axum::{extract::State, response::Json};
use chrono::Utc;

/// 时间同步响应
#[derive(Serialize)]
pub struct TimeSyncResponse {
    /// 服务器当前时间 (毫秒)
    pub server_time: i64,

    /// 服务器接收请求时间 (毫秒)
    pub server_recv_time: i64,

    /// 服务器发送响应时间 (毫秒)
    pub server_send_time: i64,

    /// 请求ID (用于去重)
    pub request_id: String,
}

/// GET /api/v1/time/sync - 时间同步端点
#[utoipa::path(
    get,
    path = "/api/v1/time/sync",
    tag = "时间同步",
    summary = "获取服务器时间用于同步",
    responses(
        (status = 200, description = "成功", body = ApiResponse<TimeSyncResponse>)
    )
)]
pub async fn time_sync(
    State(state): State<AppState>,
) -> Result<Json<ApiResponse<TimeSyncResponse>>> {
    // 记录接收时间 (尽早记录)
    let recv_time = Utc::now().timestamp_millis();

    // 处理逻辑 (实际上这个端点很简单,几乎无处理时间)
    let request_id = Uuid::new_v4().to_string();

    // 记录发送时间 (尽晚记录)
    let send_time = Utc::now().timestamp_millis();

    Ok(Json(ApiResponse::success(TimeSyncResponse {
        server_time: send_time,
        server_recv_time: recv_time,
        server_send_time: send_time,
        request_id,
    })))
}
```

### 2.3 时间同步策略

#### 2.3.1 同步时机

客户端应在以下时机进行时间同步：

1. 页面加载时立即同步（5次测量）
2. 定期检查并重新同步（每5分钟，3次测量）
3. 网络状态变化时重新同步
4. 检测到时间跳变时重新同步（检测间隔超出预期±500ms）

#### 2.3.2 多副本时间一致性

**问题**: 客户端可能连接到不同副本,每个副本的时间是否一致?

**解决方案**:

- ✅ **服务端依赖系统NTP** - 所有服务器通过系统级NTP同步时间
- ✅ **应用层不处理** - Kubernetes/Docker环境自动配置时间同步
- ✅ **假设服务端时间准确** - 服务端之间时间误差 < 10ms (NTP保证)

```yaml
# Kubernetes时间同步配置 (通常由运维配置)
apiVersion: v1
kind: Pod
spec:
  hostNetwork: true  # 使用宿主机网络,共享宿主机时间

  # 或者挂载宿主机时区
  volumes:
  - name: tz-config
    hostPath:
      path: /etc/localtime
  volumeMounts:
  - name: tz-config
    mountPath: /etc/localtime
    readOnly: true
```

**监控时间偏移**:

```rust
use prometheus::{Histogram, register_histogram};

lazy_static! {
    static ref TIME_SYNC_OFFSET: Histogram = register_histogram!(
        "time_sync_offset_ms",
        "Client time offset in milliseconds"
    ).unwrap();
}

pub async fn time_sync(State(state): State<AppState>) -> Result<Json<ApiResponse<TimeSyncResponse>>> {
    let recv_time = Utc::now().timestamp_millis();
    // ... 处理 ...

    // 记录时间偏移到监控 (从请求头获取客户端时间)
    if let Some(client_time) = extract_client_time_from_header() {
        let offset = recv_time - client_time;
        TIME_SYNC_OFFSET.observe(offset as f64);
    }

    // ...
}
```

### 2.4 精度与误差

| 指标 | 目标值 | 说明 |
|------|--------|------|
| 客户端时钟偏移 | ±100ms | HTTP往返延迟较大,但足够播放同步 |
| 服务端时钟偏移 | ±10ms | 系统NTP保证 |
| 播放同步精度 | ±300ms | 人眼察觉阈值 |
| 时间同步频率 | 5分钟 | 避免频繁请求,客户端时钟通常稳定 |

**注意**: HTTP时间同步精度低于NTP (NTP可达1-10ms),但对于视频播放同步已经足够

---

## 3. 时间补偿原理

### 3.1 客户端时间戳设计

**核心思想**: 客户端发送操作时附带**同步后的服务器时间戳**,服务端计算网络延迟并补偿

```rust
/// WebSocket消息结构
pub struct PlayerControlMessage {
    /// 消息类型 (play, pause, seek)
    pub action: PlayerAction,

    /// 客户端发送时间戳 (毫秒) - 使用同步后的服务器时间！
    /// 通过 timeSync.getServerTime() 获取
    pub timestamp: i64,

    /// 视频当前位置 (秒)
    pub position: f64,

    /// 播放速度 (1.0 = 正常速度)
    pub rate: f64,
}

#[derive(Serialize, Deserialize)]
pub enum PlayerAction {
    Play,
    Pause,
    Seek,
}
```

### 3.2 服务端延迟计算

```rust
/// 计算网络延迟并补偿
///
/// 前提: 客户端通过 HTTP 时间同步获得时钟偏移,发送的是同步后的服务器时间
///
/// 算法:
/// 1. 计算消息在网络中的传输时间
/// 2. 限制最大补偿时间 (避免异常值)
/// 3. 返回补偿后的时间差
pub fn calculate_time_diff(client_timestamp: i64) -> f64 {
    if client_timestamp == 0 {
        return 0.0;
    }

    // 服务器当前时间
    let now = Utc::now().timestamp_millis();

    // 网络传输时间 (毫秒)
    // 因为客户端已经同步时钟,这里就是真实的网络延迟
    let network_delay = now - client_timestamp;

    // 异常情况: 时间戳晚于服务器
    // 可能原因:
    // 1. 客户端时间同步失败或过期
    // 2. 客户端时间被手动修改
    // 3. 网络时间错乱
    if network_delay < 0 {
        tracing::warn!(
            "Client timestamp ahead of server: {}ms (possible clock desync)",
            network_delay.abs()
        );
        return 0.0;
    }

    // 转换为秒
    let time_diff = network_delay as f64 / 1000.0;

    // 限制最大补偿时间: 1.5秒
    // 理由:
    // - 正常网络延迟: 10-500ms
    // - 异常网络: 500-1500ms
    // - 超过1.5秒说明网络极差或客户端时间同步失效,限制补偿避免跳跃过大
    let compensated = time_diff.min(1.5);

    // 记录监控指标
    metrics::histogram!("player_control_latency_ms", network_delay as f64);
    if network_delay > 500 {
        tracing::warn!("High network latency detected: {}ms", network_delay);
    }

    compensated
}
```

### 3.3 客户端发送时间戳

**重要**: 客户端发送播放控制消息时，必须使用同步后的服务器时间，而非本地时间 `Date.now()`。

时间戳字段说明：

- `timestamp`: 使用 `timeSync.getServerTime()` 获取的同步后服务器时间
- `position`: 视频当前播放位置（秒）
- `rate`: 播放速率（默认1.0）

### 3.4 时间同步流程图

```
步骤1: 客户端A初始化时进行时间同步
   客户端A  ────  GET /api/v1/time/sync  ───→  服务器
   客户端A  ←─── { server_time, recv, send } ─  服务器
   计算偏移: offset = ((t2-t1) + (t3-t4)) / 2
   保存偏移: offsetMs = -50ms (客户端慢50ms)

步骤2: 客户端A点击播放
   客户端A本地时间: 1000ms
   同步后时间: 1000 + (-50) = 950ms  ← 使用这个!

步骤3: 发送播放消息
   客户端A  ─── play(timestamp=950, pos=10.0) ─→  服务器

步骤4: 服务器计算延迟并补偿
   服务器当前时间: 1000ms
   网络延迟: 1000 - 950 = 50ms
   补偿位置: 10.0 + 0.05s = 10.05s

步骤5: 广播到其他客户端
   服务器  ─── play(timestamp=1000, pos=10.05) ─→  客户端B

步骤6: 客户端B接收并补偿接收延迟
   接收时间: 1050ms
   接收延迟: 1050 - 1000 = 50ms
   最终位置: 10.05 + 0.05 = 10.10s
```

**关键点**:

- ✅ 客户端使用 `timeSync.getServerTime()` 而非 `Date.now()`
- ✅ 服务器计算的延迟是真实的网络延迟 (已消除时钟偏移)
- ✅ 双向补偿: 发送延迟 + 接收延迟

---

## 4. 补偿算法

### 4.1 播放状态补偿

```rust
/// 处理播放控制消息
pub async fn handle_player_control(
    client: &Client,
    msg: PlayerControlMessage,
) -> Result<()> {
    // 1. 计算网络延迟
    let time_diff = calculate_time_diff(msg.timestamp);

    // 2. 根据不同操作进行补偿
    let compensated_position = match msg.action {
        PlayerAction::Play => {
            // 播放: 补偿网络延迟
            // 用户点击播放时位置是 10.0s
            // 消息到达服务器已经过了 0.05s
            // 其他用户应该从 10.05s 开始播放
            msg.position + time_diff * msg.rate
        }
        PlayerAction::Pause => {
            // 暂停: 同样补偿网络延迟
            msg.position + time_diff * msg.rate
        }
        PlayerAction::Seek => {
            // 跳转: 不补偿 (用户明确指定位置)
            msg.position
        }
    };

    // 3. 广播到其他客户端
    client.room().broadcast_player_state(
        PlayerState {
            action: msg.action,
            position: compensated_position,
            rate: msg.rate,
            timestamp: Utc::now().timestamp_millis(),
        },
        Some(client.id()),  // 排除发送者
    ).await?;

    Ok(())
}
```

### 4.2 断线重连补偿

```rust
/// 客户端断线重连时,计算当前应该播放的位置
pub fn calculate_reconnect_position(
    last_state: &PlayerState,
    reconnect_time: i64,
) -> f64 {
    if last_state.action == PlayerAction::Pause {
        // 暂停状态: 直接返回暂停位置
        return last_state.position;
    }

    // 播放状态: 计算应该播放到的位置
    let elapsed = (reconnect_time - last_state.timestamp) as f64 / 1000.0;
    last_state.position + elapsed * last_state.rate
}

/// 处理重连
pub async fn handle_reconnect(client: &Client) -> Result<()> {
    let room = client.room();
    let last_state = room.get_player_state();

    let current_position = calculate_reconnect_position(
        &last_state,
        Utc::now().timestamp_millis(),
    );

    // 发送当前播放状态
    client.send(PlayerStateMessage {
        action: last_state.action,
        position: current_position,
        rate: last_state.rate,
    }).await?;

    Ok(())
}
```

### 4.3 多副本时间同步

**挑战**: 多副本架构下,客户端可能连接到不同副本

**解决方案**: 使用 Redis 存储全局播放状态

```rust
/// 全局播放状态 (存储在 Redis)
#[derive(Serialize, Deserialize)]
pub struct GlobalPlayerState {
    pub room_id: String,
    pub action: PlayerAction,
    pub position: f64,
    pub rate: f64,
    pub timestamp: i64,
    pub updated_by: String,  // 副本ID
}

impl GlobalPlayerState {
    /// 保存到 Redis
    pub async fn save(&self, redis: &RedisPool) -> Result<()> {
        let key = format!("room:{}:player_state", self.room_id);
        let value = serde_json::to_string(self)?;

        redis.set_ex(&key, value, 300).await?;  // 5分钟过期

        // 发布变更通知 (通知其他副本)
        redis.publish(
            "player_state:changed",
            serde_json::to_string(self)?,
        ).await?;

        Ok(())
    }

    /// 从 Redis 加载
    pub async fn load(
        redis: &RedisPool,
        room_id: &str,
    ) -> Result<Option<Self>> {
        let key = format!("room:{}:player_state", room_id);
        let value: Option<String> = redis.get(&key).await?;

        match value {
            Some(v) => Ok(Some(serde_json::from_str(&v)?)),
            None => Ok(None),
        }
    }

    /// 获取当前位置 (考虑时间流逝)
    pub fn current_position(&self) -> f64 {
        if self.action == PlayerAction::Pause {
            return self.position;
        }

        let elapsed = (Utc::now().timestamp_millis() - self.timestamp) as f64 / 1000.0;
        self.position + elapsed * self.rate
    }
}
```

---

## 5. 实现方案

### 5.1 客户端实现要点

客户端实现需要包含以下关键步骤：

1. **初始化时进行时间同步**
   - 执行5次测量取中位数
   - 记录时钟偏移量和往返延迟

2. **发送播放控制时使用同步后时间**
   - 使用 `getServerTime()` 而非本地时间
   - 附带当前播放位置和速率

3. **接收播放状态时补偿接收延迟**
   - 计算接收延迟: `now - server_timestamp`
   - 播放状态需补偿: `position + (receiveLatency / 1000) * rate`
   - 暂停状态直接使用position

4. **定期重新同步**
   - 每5分钟检查并重新同步
   - 检测时间跳变时立即重新同步

### 5.2 WebSocket 消息定义

```protobuf
// proto/player.proto

message PlayerControl {
    enum Action {
        PLAY = 0;
        PAUSE = 1;
        SEEK = 2;
    }

    Action action = 1;
    int64 timestamp = 2;  // 客户端时间戳 (毫秒)
    double position = 3;  // 视频位置 (秒)
    double rate = 4;      // 播放速度 (默认1.0)
}

message PlayerState {
    enum Action {
        PLAY = 0;
        PAUSE = 1;
    }

    Action action = 1;
    double position = 2;
    double rate = 3;
    int64 server_timestamp = 4;  // 服务器时间戳
}
```

### 5.3 服务端完整实现

```rust
use dashmap::DashMap;
use std::sync::Arc;

pub struct RoomPlayerManager {
    states: Arc<DashMap<String, PlayerState>>,
    redis: RedisPool,
}

impl RoomPlayerManager {
    pub async fn handle_player_control(
        &self,
        room_id: &str,
        client_id: &str,
        msg: PlayerControlMessage,
    ) -> Result<()> {
        // 1. 计算补偿
        let time_diff = calculate_time_diff(msg.timestamp);

        let compensated_position = match msg.action {
            PlayerAction::Play | PlayerAction::Pause => {
                msg.position + time_diff * msg.rate
            }
            PlayerAction::Seek => msg.position,
        };

        // 2. 创建新状态
        let new_state = PlayerState {
            action: msg.action,
            position: compensated_position,
            rate: msg.rate,
            timestamp: Utc::now().timestamp_millis(),
        };

        // 3. 保存到本地缓存
        self.states.insert(room_id.to_string(), new_state.clone());

        // 4. 保存到 Redis (多副本同步)
        let global_state = GlobalPlayerState {
            room_id: room_id.to_string(),
            action: new_state.action,
            position: new_state.position,
            rate: new_state.rate,
            timestamp: new_state.timestamp,
            updated_by: get_local_node_id(),
        };
        global_state.save(&self.redis).await?;

        // 5. 广播到房间内所有客户端
        self.broadcast_player_state(room_id, &new_state, Some(client_id))
            .await?;

        Ok(())
    }

    pub async fn get_current_position(&self, room_id: &str) -> Result<f64> {
        // 1. 尝试从本地缓存获取
        if let Some(state) = self.states.get(room_id) {
            return Ok(state.current_position());
        }

        // 2. 从 Redis 获取
        if let Some(global_state) = GlobalPlayerState::load(&self.redis, room_id).await? {
            let position = global_state.current_position();

            // 更新本地缓存
            self.states.insert(
                room_id.to_string(),
                PlayerState {
                    action: global_state.action,
                    position: global_state.position,
                    rate: global_state.rate,
                    timestamp: global_state.timestamp,
                },
            );

            return Ok(position);
        }

        // 3. 默认: 从头开始
        Ok(0.0)
    }
}
```

---

## 6. 性能优化

### 6.1 批量处理

```rust
use tokio::time::{interval, Duration};

/// 批量处理播放状态更新 (减少Redis写入)
pub async fn start_batch_updater(manager: Arc<RoomPlayerManager>) {
    let mut interval = interval(Duration::from_secs(1));

    loop {
        interval.tick().await;

        // 每秒批量更新到Redis
        let mut batch = Vec::new();
        for entry in manager.states.iter() {
            batch.push((entry.key().clone(), entry.value().clone()));
        }

        if let Err(e) = manager.batch_update_redis(&batch).await {
            tracing::error!("Failed to batch update Redis: {}", e);
        }
    }
}
```

### 6.2 时间戳缓存

```rust
/// 缓存最近的时间戳,减少系统调用
pub struct TimestampCache {
    cached_time: Arc<AtomicI64>,
}

impl TimestampCache {
    pub fn new() -> Self {
        let cached_time = Arc::new(AtomicI64::new(Utc::now().timestamp_millis()));
        let cached_time_clone = cached_time.clone();

        // 后台线程每10ms更新一次
        tokio::spawn(async move {
            let mut interval = interval(Duration::from_millis(10));
            loop {
                interval.tick().await;
                let now = Utc::now().timestamp_millis();
                cached_time_clone.store(now, Ordering::Relaxed);
            }
        });

        Self { cached_time }
    }

    pub fn now(&self) -> i64 {
        self.cached_time.load(Ordering::Relaxed)
    }
}
```

### 6.3 自适应补偿

```rust
/// 根据网络状况自适应调整补偿策略
pub struct AdaptiveCompensation {
    recent_latencies: Arc<Mutex<VecDeque<f64>>>,
    max_samples: usize,
}

impl AdaptiveCompensation {
    pub fn new() -> Self {
        Self {
            recent_latencies: Arc::new(Mutex::new(VecDeque::with_capacity(100))),
            max_samples: 100,
        }
    }

    pub async fn record_latency(&self, latency: f64) {
        let mut latencies = self.recent_latencies.lock().await;
        latencies.push_back(latency);
        if latencies.len() > self.max_samples {
            latencies.pop_front();
        }
    }

    pub async fn calculate_compensation(&self, measured_latency: f64) -> f64 {
        let latencies = self.recent_latencies.lock().await;

        if latencies.is_empty() {
            return measured_latency.min(1.5);
        }

        // 计算P95延迟
        let mut sorted: Vec<f64> = latencies.iter().copied().collect();
        sorted.sort_by(|a, b| a.partial_cmp(b).unwrap());
        let p95_latency = sorted[(sorted.len() as f64 * 0.95) as usize];

        // 如果当前延迟远大于P95,可能是异常,限制补偿
        if measured_latency > p95_latency * 3.0 {
            return p95_latency.min(1.5);
        }

        measured_latency.min(1.5)
    }
}
```

---

## 7. 总结

本章节设计了一个**完整、精确**的时间同步与补偿系统:

### 7.1 核心特性

✅ **HTTP时间同步**: 客户端启动时通过HTTP协议同步时钟,消除时钟偏移
✅ **类NTP算法**: 使用往返时间测量,多次采样取中位数,精度±100ms
✅ **双向补偿**: 发送延迟补偿 + 接收延迟补偿
✅ **多副本支持**: 服务端通过系统NTP同步,Redis存储全局播放状态
✅ **断线重连**: 自动计算当前应播放位置
✅ **定期重同步**: 每5分钟检查并重新同步,应对时间漂移
✅ **性能优化**: 批量处理、时间戳缓存、自适应补偿
✅ **误差控制**: 保证用户间播放误差 < 300ms

### 7.2 关键技术点

| 技术点 | 实现方式 | 目标 |
|--------|---------|------|
| 客户端时钟同步 | HTTP往返测量,类NTP算法 | ±100ms精度 |
| 服务端时钟同步 | 系统NTP (运维配置) | ±10ms精度 |
| 时间戳发送 | `timeSync.getServerTime()` | 消除时钟偏移 |
| 延迟补偿 | 发送/接收双向补偿 | 实时追踪 |
| 异常处理 | 限制最大补偿1.5秒 | 避免跳跃 |
| 定期同步 | 5分钟重新同步 | 应对时间漂移 |

### 7.3 监控指标

```rust
// Prometheus监控指标
- time_sync_offset_ms: 客户端时钟偏移分布
- player_control_latency_ms: 播放控制消息延迟
- time_sync_requests_total: 时间同步请求数
- time_sync_errors_total: 时间同步失败数
```

### 7.4 部署注意事项

1. **服务端时间同步** (运维必须配置):

   ```bash
   # 检查NTP服务状态
   systemctl status systemd-timesyncd

   # 检查时间同步
   timedatectl status
   ```

2. **Kubernetes配置**:

   ```yaml
   # 使用宿主机时间
   spec:
     hostNetwork: true
   ```

3. **监控告警**:
   - 客户端时钟偏移 > 5秒 → 警告
   - 服务端时钟偏移 > 100ms → 告警
   - 播放延迟 > 1秒 → 警告

---

**上一章**: [11-实时互动](./11-实时互动.md)
**下一章**: [13-自动连播设计](./10-视频链接过期与代理机制.md)
