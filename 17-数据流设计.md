# 11. 数据流设计

---

## 11.1 推流数据流

用户推流到SyncTV后，系统需要通过Publisher节点转发给其他副本节点，实现流的多点分发。

### 11.1.1 RTMP推流完整流程

```
用户推流端
    │
    │ 1. RTMP Publish
    │    rtmp://node-2.synctv.io/live/room_123_abc123
    ▼
┌─────────────────────────────────────┐
│  Node-2 (Xiu RTMP Server)           │
│                                     │
│  ┌──────────────────────────────┐  │
│  │ RTMP Handler                 │  │
│  │ - 接收握手                   │  │
│  │ - 验证stream_key             │  │
│  └──────────┬───────────────────┘  │
│             │                       │
│             │ 2. 原子注册到Redis    │
│             │    (HSETNX)          │
│             ▼                       │
│  ┌──────────────────────────────┐  │
│  │ Redis Registry               │  │
│  │ SET stream:publisher:room_123│  │
│  │     {"node_id": "node-2"}   │  │
│  │     EX 300                   │  │
│  └──────────┬───────────────────┘  │
│             │                       │
│             │ 4. (可选) 发布事件用于监控│
│             ▼                       │
│  ┌──────────────────────────────┐  │
│  │ Redis Pub/Sub (可选)         │  │
│  │ PUBLISH cluster:events       │  │
│  │   StreamStarted { ... }      │  │
│  │ (仅用于监控和统计)           │  │
│  └──────────────────────────────┘  │
└─────────────────────────────────────┘

注意: 其他节点不会主动拉流!
      而是在收到客户端FLV请求时才懒加载创建拉流

┌──────────────────────┬──────────────────────┐
│                      │                      │
▼                      ▼                      ▼
Node-1               Node-3               Node-4
│                      │                      │
│ 等待客户端请求       │                      │
│ (懒加载模式)         │                      │
└──────────────────────┴──────────────────────┘
```

### 11.1.2 流数据转发时序图

**Publisher节点 (Node-2)**

```rust
pub async fn handle_rtmp_publish(
    stream_key: String,
    rtmp_stream: RtmpStream,
    stream_manager: Arc<StreamManager>,
    cluster_sync: Arc<ClusterSync>,
) -> Result<()> {
    // 1. 验证stream_key
    let room_id = validate_stream_key(&stream_key).await?;

    // 2. 原子注册为Publisher (HSETNX)
    //    任何副本都可以接受推流，通过 Redis 原子操作保证全局唯一
    let registration = stream_manager
        .try_register_publisher(&stream_key)
        .await?;

    // 3. 启动心跳维护
    stream_manager.maintain_publisher_heartbeat(stream_key.clone()).await;

    // 4. (可选) 发布事件用于监控和统计
    //    注意: 不用于节点间协调!
    //          其他节点通过 Redis 查询 Publisher，在收到客户端FLV请求时才懒加载创建拉流
    cluster_sync.publish_event(ClusterEvent::StreamStarted {
        stream_key: stream_key.clone(),
        publisher_node_id: registration.node_id.clone(),
        room_id: room_id.clone(),
        timestamp: Utc::now().timestamp_millis(),
    }).await.ok();  // 允许失败，不影响推流

    // 5. 启动流处理（GOP缓存、HLS切片生成等）
    let stream_processor = StreamProcessor::new(stream_key.clone());
    let result = stream_processor.process(rtmp_stream).await;

    // 6. 流结束后注销Publisher
    stream_manager.unregister_publisher(&stream_key).await?;

    result

    Ok(())
}
```

**副本节点 (Node-1, Node-3, Node-4) - 懒加载模式**

在懒加载模式下,副本节点不监听 StreamStarted 事件,而是在收到客户端 FLV 请求时才创建拉流:

```rust
// 懒加载: 收到客户端FLV请求时才创建拉流
pub async fn handle_flv_request(
    Path(room_id): Path<String>,
    State(state): State<AppState>,
) -> Result<Response> {
    let stream_key = format!("room_{}", room_id);

    // 1. 查询 Redis 获取 Publisher 节点
    let publisher_info = state.redis
        .hgetall(format!("stream:{}", stream_key))
        .await?;

    let publisher_node = publisher_info
        .get("publisher_node")
        .ok_or(Error::StreamNotFound)?;

    // 2. 懒加载: 获取或创建拉流 (首次请求时创建)
    let pull_stream = state.pull_stream_manager
        .get_or_create_pull_stream(&stream_key, publisher_node)
        .await?;

    // 3. 订阅 FLV 流
    let flv_stream = pull_stream.subscribe_flv().await?;

    // 4. 返回流式响应
    Ok(Response::builder()
        .header("Content-Type", "video/x-flv")
        .header("X-Pull-Mode", "lazy-load")
        .body(Body::from_stream(flv_stream))?)
}
```

**懒加载优势**:

- ✅ 按需创建拉流,节省资源
- ✅ 无需监听集群事件
- ✅ 首次请求时自动建立 RTMP Client 连接
- ✅ 自动清理无订阅者的拉流 (5 分钟超时)

### 11.1.3 Publisher冲突预防

#### 问题背景

在多副本架构下，如果多个副本同时处理同一流的推流请求，可能导致：

- 多个副本都认为自己是Publisher
- 流数据冲突，客户端收到混乱的数据
- HLS切片损坏

#### 原子注册机制

使用Redis `HSETNX`原子操作注册Publisher，保证全局唯一性：

```rust
// live/stream_manager.rs
pub struct StreamManager {
    redis: deadpool_redis::Pool,
    local_node_id: String,
}

impl StreamManager {
    /// 尝试注册为Publisher（原子操作）
    pub async fn try_register_publisher(
        &self,
        stream_key: &str,
    ) -> Result<PublisherRegistration> {
        let key = format!("stream:{}", stream_key);
        let mut conn = self.redis.get().await?;

        // 使用 HSETNX 原子操作注册publisher_node
        let registered: bool = redis::cmd("HSETNX")
            .arg(&key)
            .arg("publisher_node")
            .arg(&self.local_node_id)
            .query_async(&mut conn)
            .await?;

        if !registered {
            // 已有Publisher，获取现有Publisher信息
            let existing: Option<String> = redis::cmd("HGET")
                .arg(&key)
                .arg("publisher_node")
                .query_async(&mut conn)
                .await?;

            return Err(Error::PublisherAlreadyExists {
                stream_key: stream_key.to_string(),
                existing_node: existing.unwrap_or_default(),
            });
        }

        // 设置其他字段
        redis::cmd("HSET")
            .arg(&key)
            .arg("stream_key")
            .arg(stream_key)
            .arg("started_at")
            .arg(Utc::now().timestamp())
            .arg("status")
            .arg("live")
            .arg("viewer_count")
            .arg("0")
            .arg("last_heartbeat")
            .arg(Utc::now().timestamp())
            .query_async(&mut conn)
            .await?;

        // 设置TTL（5分钟）
        redis::cmd("EXPIRE")
            .arg(&key)
            .arg(300)
            .query_async(&mut conn)
            .await?;

        Ok(PublisherRegistration {
            stream_key: stream_key.to_string(),
            node_id: self.local_node_id.clone(),
            started_at: Utc::now(),
        })
    }

    /// 维持Publisher心跳
    pub async fn maintain_publisher_heartbeat(
        &self,
        stream_key: String,
    ) {
        let redis = self.redis.clone();
        let key = format!("stream:{}", stream_key);

        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(30));

            loop {
                interval.tick().await;

                let result: Result<()> = async {
                    let mut conn = redis.get().await?;

                    // 更新心跳时间戳
                    redis::cmd("HSET")
                        .arg(&key)
                        .arg("last_heartbeat")
                        .arg(Utc::now().timestamp())
                        .query_async(&mut conn)
                        .await?;

                    // 刷新TTL
                    redis::cmd("EXPIRE")
                        .arg(&key)
                        .arg(300)
                        .query_async(&mut conn)
                        .await?;

                    Ok(())
                }.await;

                if result.is_err() {
                    tracing::error!(
                        stream_key = %stream_key,
                        "Failed to maintain heartbeat, stopping"
                    );
                    break;
                }
            }
        });
    }

    /// 注销Publisher
    pub async fn unregister_publisher(
        &self,
        stream_key: &str,
    ) -> Result<()> {
        let key = format!("stream:{}", stream_key);
        let mut conn = self.redis.get().await?;

        // 删除整个hash
        redis::cmd("DEL")
            .arg(&key)
            .query_async(&mut conn)
            .await?;

        tracing::info!(
            stream_key = %stream_key,
            node_id = %self.local_node_id,
            "Publisher unregistered"
        );

        Ok(())
    }
}

#[derive(Debug)]
pub struct PublisherRegistration {
    pub stream_key: String,
    pub node_id: String,
    pub started_at: DateTime<Utc>,
}
```

#### 推流处理流程（改进版）

```rust
pub async fn handle_rtmp_publish_improved(
    stream_key: String,
    rtmp_stream: RtmpStream,
    stream_manager: Arc<StreamManager>,
    cluster_sync: Arc<ClusterSync>,
) -> Result<()> {
    // 1. 验证stream_key
    let room_id = validate_stream_key(&stream_key).await?;

    // 2. 尝试注册为Publisher（原子操作）
    let registration = match stream_manager
        .try_register_publisher(&stream_key)
        .await
    {
        Ok(reg) => reg,
        Err(Error::PublisherAlreadyExists { existing_node, .. }) => {
            // 已有Publisher，拒绝推流
            tracing::warn!(
                stream_key = %stream_key,
                existing_node = %existing_node,
                "Publisher already exists, rejecting publish"
            );

            return Err(Error::PublisherConflict {
                message: format!(
                    "Stream {} already has a publisher on node {}",
                    stream_key, existing_node
                ),
            });
        }
        Err(e) => return Err(e),
    };

    tracing::info!(
        stream_key = %stream_key,
        node_id = %registration.node_id,
        "Publisher registered successfully"
    );

    // 3. 启动心跳维护
    stream_manager.maintain_publisher_heartbeat(stream_key.clone()).await;

    // 4. (可选) 发布监控事件
    //    不用于节点间协调，仅用于监控和统计
    cluster_sync.publish(ClusterEvent::StreamStarted {
        stream_key: stream_key.clone(),
        publisher_node_id: registration.node_id.clone(),
        room_id: room_id.clone(),
        timestamp: Utc::now().timestamp_millis(),
    }).await.ok();  // 允许失败

    // 5. 启动流处理
    let stream_processor = StreamProcessor::new(stream_key.clone());

    let result = stream_processor.process(rtmp_stream).await;

    // 6. 流结束后注销Publisher
    stream_manager.unregister_publisher(&stream_key).await?;

    // 7. 通知流停止
    cluster_sync.publish(ClusterEvent::StreamStopped {
        stream_key,
        publisher_node_id: registration.node_id,
        stopped_at: Utc::now(),
    }).await?;

    result
}
```

#### Redis数据结构

```
# Publisher记录 (Hash)
stream:{stream_key}
    publisher_node: "node-2"
    stream_key: "room_123_abc123"
    started_at: 1705312800
    status: "live"
    viewer_count: "123"
    last_heartbeat: 1705312950

# 设置过期时间
EXPIRE stream:{stream_key} 300  # 5分钟
```

#### 心跳机制

Publisher每30秒更新一次心跳，刷新Redis键的TTL：

```rust
// 心跳任务
tokio::spawn(async move {
    let mut interval = tokio::time::interval(Duration::from_secs(30));

    loop {
        interval.tick().await;

        // 更新last_heartbeat和TTL
        let result = update_heartbeat(&stream_key).await;

        if result.is_err() {
            // 心跳失败，停止任务
            break;
        }
    }
});
```

**超时检测**：

- Redis键5分钟自动过期
- 心跳间隔30秒，允许10次重试失败
- Publisher崩溃后，最多5分钟内自动清理

#### 错误处理

```rust
// 错误类型
pub enum Error {
    /// Publisher已存在
    PublisherAlreadyExists {
        stream_key: String,
        existing_node: String,
    },

    /// Publisher冲突
    PublisherConflict {
        message: String,
    },

    // ... 其他错误
}

// 客户端推流被拒绝时的响应
impl Error {
    pub fn to_rtmp_status(&self) -> RtmpStatus {
        match self {
            Error::PublisherAlreadyExists { existing_node, .. } => {
                RtmpStatus::error(
                    "NetStream.Publish.BadName",
                    format!("Stream already published on {}", existing_node)
                )
            }
            _ => RtmpStatus::error("NetStream.Publish.Failed", "Unknown error"),
        }
    }
}
```

#### 设计要点

**原子性**：

- 使用`HSETNX`保证只有一个副本能注册成功
- 避免多个Publisher同时写入同一流

**故障恢复**：

- TTL机制：Publisher崩溃后自动清理（最多5分钟）
- 心跳机制：持续验证Publisher存活
- 推流端重试：被拒绝后可等待重试

**性能优化**：

- 心跳间隔30秒，减少Redis压力
- 使用Hash存储多个字段，减少键数量

**监控指标**：

- Publisher注册失败次数
- 心跳失败次数
- TTL过期事件

---

## 11.2 拉流数据流 (懒加载模式)

客户端从任意节点请求 FLV 时,该节点会懒加载创建拉流连接,从 Publisher 拉取 RTMP 流并在本地转码为 FLV 分发。

**关键设计**:

- ✅ **懒加载**: 收到客户端请求时才创建拉流
- ✅ **连接共享**: 同节点多客户端共享一个拉流连接
- ✅ **自动清理**: 无订阅者 5 分钟后自动停止拉流

### 11.2.1 HLS拉流流程 (透传模式)

HLS 采用透传 Publisher 或 OSS 模式,不需要 gRPC 转发:

```
客户端请求播放列表
    │
    │ GET https://node-1.synctv.io/hls/room_123_abc123/index.m3u8
    ▼
┌─────────────────────────────────────┐
│  Node-1 (任意节点)                   │
│                                     │
│  ┌──────────────────────────────┐  │
│  │ HLS Handler                  │  │
│  │ - 解析stream_key             │  │
│  └──────────┬───────────────────┘  │
│             │                       │
│             │ 1. 查询Publisher节点  │
│             ▼                       │
│  ┌──────────────────────────────┐  │
│  │ Redis Lookup                 │  │
│  │ GET stream:publisher:room_123│  │
│  │ -> "node-2"                  │  │
│  └──────────┬───────────────────┘  │
│             │                       │
│             │ 2. 检查 OSS 存储      │
│             ▼                       │
│  ┌──────────────────────────────┐  │
│  │ OSS Storage Check            │  │
│  │ GET hls/room_123/index.m3u8  │  │
│  └──────────┬───────────────────┘  │
│             │                       │
│             ├─ 存在 → 302 重定向     │
│             │   Location: OSS URL  │
│             │                       │
│             └─ 不存在 → 透传Publisher│
│                 ▼                   │
│  ┌──────────────────────────────┐  │
│  │ HTTP Proxy                   │  │
│  │ GET http://node-2/hls/...    │  │
│  └──────────┬───────────────────┘  │
│             │                       │
│             │ 3. 返回给客户端       │
│             ▼                       │
│  ┌──────────────────────────────┐  │
│  │ HTTP Response                │  │
│  │ X-Publisher-Node: node-2     │  │
│  └──────────────────────────────┘  │
└─────────────────────────────────────┘
              │
              ▼
          客户端
```

**优势**:

- ✅ 确保所有客户端获取的切片一致
- ✅ 支持 OSS + CDN 加速
- ✅ 无需 gRPC,简化架构
- ✅ 可选本地缓存 (1 分钟 TTL)

### 11.2.2 FLV拉流流程 (懒加载)

FLV 采用懒加载模式: 节点在收到第一个客户端 FLV 请求时,才创建 RTMP Client 连接到 Publisher 拉取流:

```
客户端请求FLV
    │
    │ GET https://node-3.synctv.io/flv/room_123_abc123.flv
    ▼
┌─────────────────────────────────────┐
│  Node-3                             │
│                                     │
│  ┌──────────────────────────────┐  │
│  │ FLV Handler                  │  │
│  └──────────┬───────────────────┘  │
│             │                       │
│             │ 1. 检查本地GOP缓存    │
│             ▼                       │
│  ┌──────────────────────────────┐  │
│  │ GOP Cache                    │  │
│  │ ✓ 有缓存数据                 │  │
│  └──────────┬───────────────────┘  │
│             │                       │
│             │ 2. 发送FLV头 + GOP    │
│             ▼                       │
│  ┌──────────────────────────────┐  │
│  │ FLV Writer                   │  │
│  │ - FLV Header                 │  │
│  │ - Metadata                   │  │
│  │ - GOP Cache Tags             │  │
│  └──────────┬───────────────────┘  │
│             │                       │
│             │ 3. 持续推送实时数据   │
│             ▼                       │
│  ┌──────────────────────────────┐  │
│  │ Live Stream Subscription     │  │
│  │ 订阅本地流分发               │  │
│  └──────────────────────────────┘  │
└─────────────────────────────────────┘
              │
              │ HTTP Chunked Transfer
              ▼
          客户端（实时接收）
```

---

## 11.3 房间同步数据流

房间中的播放控制、聊天消息等需要通过Redis Pub/Sub实现跨节点实时同步。

### 11.3.1 播放控制同步

```
用户A (在Node-1连接)
    │
    │ WebSocket: {"type": "playback.toggle", "data": {"action": "play"}}
    ▼
┌─────────────────────────────────────┐
│  Node-1                             │
│                                     │
│  ┌──────────────────────────────┐  │
│  │ WebSocket Handler            │  │
│  │ - 验证权限                   │  │
│  │ - 解析消息                   │  │
│  └──────────┬───────────────────┘  │
│             │                       │
│             │ 1. 更新数据库状态     │
│             ▼                       │
│  ┌──────────────────────────────┐  │
│  │ RoomService                  │  │
│  │ UPDATE room_current_state    │  │
│  │ SET status='playing'         │  │
│  │ WHERE room_id=123            │  │
│  └──────────┬───────────────────┘  │
│             │                       │
│             │ 2. 发布Redis事件      │
│             ▼                       │
│  ┌──────────────────────────────┐  │
│  │ Redis Pub/Sub                │  │
│  │ PUBLISH room:123:events      │  │
│  │   {"type":"playback.toggle"} │  │
│  └──────────┬───────────────────┘  │
│             │                       │
│             │ 3. 本地广播           │
│             ▼                       │
│  ┌──────────────────────────────┐  │
│  │ RoomHub                      │  │
│  │ broadcast_to_room(123, msg)  │  │
│  └──────────────────────────────┘  │
└─────────────────────────────────────┘
              │
              │ Redis Pub/Sub传播
              ▼
┌───────────────┬────────────────┬───────────────┐
│               │                │               │
▼               ▼                ▼               ▼
Node-2        Node-3          Node-4         Node-5
│               │                │               │
│ 4. 订阅到事件 │                │               │
▼               ▼                ▼               ▼
RoomHub       RoomHub        RoomHub         RoomHub
│               │                │               │
│ 5. 本地广播   │                │               │
▼               ▼                ▼               ▼
WS Clients    WS Clients     WS Clients      WS Clients
(用户B,C)      (用户D)         (用户E,F,G)     (空)
```

### 11.3.2 完整代码实现

**Node-1: 处理WebSocket消息**

```rust
pub async fn handle_ws_message(
    msg: ClientMessage,
    room_id: &str,       // nanoid string
    user_id: Uuid,       // UUID
    room_service: Arc<RoomService>,
    room_hub: Arc<RoomHub>,
) -> Result<()> {
    match msg {
        ClientMessage::PlaybackToggle { action } => {
            // 1. 权限检查
            room_service.check_permission(room_id, user_id, Permission::PLAY_CONTROL).await?;

            // 2. 更新数据库
            let new_status = match action {
                PlayAction::Play => "playing",
                PlayAction::Pause => "paused",
            };

            room_service.update_playback_status(
                room_id,
                new_status,
                None,  // current_time保持不变
            ).await?;

            // 3. 发布集群事件
            let event = ClusterEvent::PlaybackToggled {
                room_id,
                status: new_status.to_string(),
                current_time: room_service.get_current_time(room_id).await?,
                user_id,
                username: room_service.get_username(user_id).await?,
                timestamp: Utc::now().timestamp_millis(),
            };

            room_hub.broadcast_cluster(room_id, event).await?;

            Ok(())
        }
        // 其他消息类型...
    }
}
```

**所有节点: 订阅Redis事件**

```rust
pub async fn subscribe_cluster_events(
    redis_client: Arc<RedisClient>,
    room_hub: Arc<RoomHub>,
) -> Result<()> {
    let mut pubsub = redis_client.get_pubsub().await?;
    pubsub.subscribe("cluster:events").await?;

    while let Some(msg) = pubsub.on_message().next().await {
        let payload: ClusterEvent = serde_json::from_slice(msg.get_payload_bytes())?;

        // 转换为WebSocket消息
        let ws_msg = match payload {
            ClusterEvent::PlaybackToggled { room_id, status, current_time, user_id, username, timestamp } => {
                ServerMessage::PlaybackStatusChanged {
                    status,
                    current_time,
                    timestamp,
                    triggered_by: UserBrief { user_id, username },
                }
            }
            // 其他事件类型...
        };

        // 广播到本节点的房间客户端
        if let ClusterEvent::PlaybackToggled { room_id, .. } = payload {
            room_hub.broadcast(room_id, ws_msg, None).await?;
        }
    }

    Ok(())
}
```

---

## 11.4 缓存数据流

系统采用多级缓存策略（L1内存 → L2 Redis → L3文件 → Origin源站），提高资源访问性能。

### 11.4.1 多级缓存读取流程

```
客户端请求资源
    │
    │ GET /api/v1/proxy?url=https://example.com/video.mp4
    ▼
┌─────────────────────────────────────┐
│  L1: 本地内存缓存 (LRU)             │
│  max_size: 1GB                      │
│  ttl: 60s                           │
└──────────┬──────────────────────────┘
           │
           │ Cache Miss
           ▼
┌─────────────────────────────────────┐
│  L2: Redis缓存                      │
│  max_size: 10GB                     │
│  ttl: 3600s                         │
└──────────┬──────────────────────────┘
           │
           │ Cache Miss
           ▼
┌─────────────────────────────────────┐
│  L3: 文件缓存                       │
│  path: /data/cache/                 │
│  max_size: 50GB                     │
│  ttl: 86400s                        │
└──────────┬──────────────────────────┘
           │
           │ Cache Miss
           ▼
┌─────────────────────────────────────┐
│  Origin: 源站                       │
│  GET https://example.com/video.mp4  │
└──────────┬──────────────────────────┘
           │
           │ 200 OK (10MB)
           ▼
      ┌─────────┐
      │ 回填缓存 │
      └─────────┘
           │
           ├──> L3: 写入文件缓存
           │
           ├──> L2: 写入Redis (如果<1MB)
           │
           ├──> L1: 写入内存 (如果<100KB)
           │
           └──> 返回给客户端
```

### 11.4.2 代码实现

```rust
pub struct MultiLevelCache {
    l1: Arc<LocalCache<String, Arc<Bytes>>>,
    l2: Arc<RedisClient>,
    l3: Arc<FileCache>,
    origin_fetcher: Arc<HttpClient>,
}

impl MultiLevelCache {
    pub async fn get(&self, key: &str) -> Result<Arc<Bytes>> {
        // L1: 内存缓存
        if let Some(data) = self.l1.get(key).await {
            return Ok(data);
        }

        // L2: Redis缓存
        if let Ok(Some(data)) = self.l2.get(key).await {
            let bytes = Arc::new(Bytes::from(data));

            // 回填L1（如果足够小）
            if bytes.len() < 100 * 1024 {
                self.l1.insert(key.to_string(), bytes.clone(), 60).await;
            }

            return Ok(bytes);
        }

        // L3: 文件缓存
        if let Ok(Some(data)) = self.l3.get(key).await {
            let bytes = Arc::new(Bytes::from(data));

            // 回填L2（如果足够小）
            if bytes.len() < 1024 * 1024 {
                self.l2.set_ex(key, bytes.as_ref(), 3600).await.ok();
            }

            // 回填L1（如果足够小）
            if bytes.len() < 100 * 1024 {
                self.l1.insert(key.to_string(), bytes.clone(), 60).await;
            }

            return Ok(bytes);
        }

        // Origin: 从源站获取
        let data = self.origin_fetcher.get(key).await?;
        let bytes = Arc::new(Bytes::from(data));

        // 写入所有缓存层
        self.write_all_levels(key, &bytes).await;

        Ok(bytes)
    }

    async fn write_all_levels(&self, key: &str, data: &Arc<Bytes>) {
        let size = data.len();

        // L3: 总是写入文件缓存
        self.l3.set(key, data.as_ref(), 86400).await.ok();

        // L2: 小于1MB写入Redis
        if size < 1024 * 1024 {
            self.l2.set_ex(key, data.as_ref(), 3600).await.ok();
        }

        // L1: 小于100KB写入内存
        if size < 100 * 1024 {
            self.l1.insert(key.to_string(), data.clone(), 60).await;
        }
    }
}
```

---

## 11.5 认证数据流

用户登录后生成JWT Token，后续请求通过JWT + Redis Session验证身份。

### 11.5.1 登录认证流程

```
客户端登录
    │
    │ POST /api/v1/auth/login
    │ {"username": "alice", "password": "***"}
    ▼
┌─────────────────────────────────────┐
│  API Server (任意节点)               │
│                                     │
│  ┌──────────────────────────────┐  │
│  │ AuthService                  │  │
│  │ 1. 查询数据库                │  │
│  │ 2. 验证密码                  │  │
│  │ 3. 生成JWT                   │  │
│  └──────────┬───────────────────┘  │
│             │                       │
│             │ JWT Token            │
│             ▼                       │
│  ┌──────────────────────────────┐  │
│  │ Redis Session                │  │
│  │ SET session:{user_id}        │  │
│  │     {"token": "...", ...}    │  │
│  │     EX 86400                 │  │
│  └──────────────────────────────┘  │
└─────────────────────────────────────┘
              │
              │ 返回Token
              ▼
          客户端保存Token
```

### 11.5.2 后续请求验证流程

```
后续请求
    │
    │ GET /api/v1/rooms
    │ Authorization: Bearer eyJhbG...
    ▼
┌─────────────────────────────────────┐
│  API Server (可能是不同节点)         │
│                                     │
│  ┌──────────────────────────────┐  │
│  │ Auth Middleware              │  │
│  │ 1. 提取Token                 │  │
│  │ 2. 验证签名                  │  │
│  └──────────┬───────────────────┘  │
│             │                       │
│             │ 3. 检查Redis会话     │
│             ▼                       │
│  ┌──────────────────────────────┐  │
│  │ Redis Session Check          │  │
│  │ GET session:{user_id}        │  │
│  │ ✓ 存在且有效                 │  │
│  └──────────┬───────────────────┘  │
│             │                       │
│             │ 4. 加载用户信息      │
│             ▼                       │
│  ┌──────────────────────────────┐  │
│  │ Request Context              │  │
│  │ req.extensions.insert(user)  │  │
│  └──────────┬───────────────────┘  │
│             │                       │
│             │ 5. 继续处理请求      │
│             ▼                       │
│  ┌──────────────────────────────┐  │
│  │ API Handler                  │  │
│  │ 可直接访问user信息           │  │
│  └──────────────────────────────┘  │
└─────────────────────────────────────┘
```

---

**上一章**: [16-实时消息流](./16-实时消息流.md)
**下一章**: [21-关键实现](./21-关键实现.md)
